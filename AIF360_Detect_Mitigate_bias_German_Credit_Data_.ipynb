{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDAx0kqGpGKcawsj7pJof2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LorenzoBelenguer/AIF360-Detect-Mitigate-bias-German-Credit-Data-/blob/main/AIF360_Detect_Mitigate_bias_German_Credit_Data_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset, German Credit Data, classifies people described by a set of attributes as good or bad credit risks. File used is german.data consisting of 1000 instances and 20 features. Detailed description of the dataset is provided in german.doc. https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29\n",
        "\n",
        "The AI Fairness 360 toolkit is an extensible open-source library containing techniques developed by the research community to help detect and mitigate bias in machine learning models throughout the AI application lifecycle. AI Fairness 360 package is available in both Python and R.\n",
        "\n",
        "The AI Fairness 360 package, AIF360, includes a comprehensive set of metrics for datasets and models to test for biases, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It is designed to translate algorithmic research from the lab into the actual practice of domains as wide-ranging as finance, human capital management, healthcare, and education.\n",
        "\n",
        "In order to check a dataset to detect and mitigate bias in AI systems using a package such as AIF360, we need at least one protected characteristic. In this example, I will be testing bias based on age.\n",
        "\n",
        "Patterns that are found may not be desirable or may even be illegal. For example, this credit score model may determine that age plays a significant role in the prediction of repayment because the training dataset happened to have better repayment for one age group than for another. This raises two problems: 1) the training dataset may not be representative of the true population of people of all age groups, and 2) even if it is representative, it is illegal to base any decision on a applicant's age, regardless of whether this is a good prediction based on historical data.\n",
        "\n",
        "If that is the case, there is a need to correct the model to reduce bias. AI360 is designed to help address this problem with fairness metrics and bias mitigators. **Fairness metrics** can be used to check for bias in machine learning workflows. **Bias mitigators** can be used to overcome bias in the workflow to produce a more fair outcome."
      ],
      "metadata": {
        "id": "wWHYGP08g_fW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.** Install the **AIF360** latest package (currently 0.5.0). Although it will not be needed it for such a basic test, I have add it two more packages (LawSchoolGPA and Reductions), just in case there is a need in the future. **Pandas** and **Numpy** also added (very popular libraries in data science)"
      ],
      "metadata": {
        "id": "vdMArs_bjIK2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cHFjR5YR07JA",
        "outputId": "42c8c9be-4af8-4071-a166-e7f5fff1a43b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aif360 in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from aif360) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from aif360) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from aif360) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360) (3.1.1)\n",
            "Requirement already satisfied: aif360[LawSchoolGPA] in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from aif360[LawSchoolGPA]) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from aif360[LawSchoolGPA]) (1.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from aif360[LawSchoolGPA]) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from aif360[LawSchoolGPA]) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from aif360[LawSchoolGPA]) (3.7.1)\n",
            "Collecting tempeh (from aif360[LawSchoolGPA])\n",
            "  Downloading tempeh-0.1.12-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->aif360[LawSchoolGPA]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->aif360[LawSchoolGPA]) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360[LawSchoolGPA]) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360[LawSchoolGPA]) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[LawSchoolGPA]) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[LawSchoolGPA]) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[LawSchoolGPA]) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[LawSchoolGPA]) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[LawSchoolGPA]) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[LawSchoolGPA]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[LawSchoolGPA]) (3.1.1)\n",
            "Collecting memory-profiler (from tempeh->aif360[LawSchoolGPA])\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from tempeh->aif360[LawSchoolGPA]) (7.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tempeh->aif360[LawSchoolGPA]) (2.31.0)\n",
            "Collecting shap (from tempeh->aif360[LawSchoolGPA])\n",
            "  Downloading shap-0.42.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.9/547.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.24.0->aif360[LawSchoolGPA]) (1.16.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory-profiler->tempeh->aif360[LawSchoolGPA]) (5.9.5)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->tempeh->aif360[LawSchoolGPA]) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->tempeh->aif360[LawSchoolGPA]) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->tempeh->aif360[LawSchoolGPA]) (1.1.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->tempeh->aif360[LawSchoolGPA]) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tempeh->aif360[LawSchoolGPA]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tempeh->aif360[LawSchoolGPA]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tempeh->aif360[LawSchoolGPA]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tempeh->aif360[LawSchoolGPA]) (2023.7.22)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap->tempeh->aif360[LawSchoolGPA]) (4.66.0)\n",
            "Collecting slicer==0.0.7 (from shap->tempeh->aif360[LawSchoolGPA])\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap->tempeh->aif360[LawSchoolGPA]) (0.56.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap->tempeh->aif360[LawSchoolGPA]) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap->tempeh->aif360[LawSchoolGPA]) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->shap->tempeh->aif360[LawSchoolGPA]) (67.7.2)\n",
            "Installing collected packages: slicer, memory-profiler, shap, tempeh\n",
            "Successfully installed memory-profiler-0.61.0 shap-0.42.1 slicer-0.0.7 tempeh-0.1.12\n",
            "Requirement already satisfied: aif360[Reductions] in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from aif360[Reductions]) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from aif360[Reductions]) (1.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from aif360[Reductions]) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from aif360[Reductions]) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from aif360[Reductions]) (3.7.1)\n",
            "Collecting fairlearn~=0.7 (from aif360[Reductions])\n",
            "  Downloading fairlearn-0.9.0-py3-none-any.whl (231 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.5/231.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->aif360[Reductions]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->aif360[Reductions]) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360[Reductions]) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->aif360[Reductions]) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[Reductions]) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[Reductions]) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[Reductions]) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[Reductions]) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[Reductions]) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[Reductions]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->aif360[Reductions]) (3.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.24.0->aif360[Reductions]) (1.16.0)\n",
            "Installing collected packages: fairlearn\n",
            "Successfully installed fairlearn-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install aif360 pandas numpy\n",
        "!pip install 'aif360[LawSchoolGPA]'\n",
        "!pip install 'aif360[Reductions]'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.** Import Statements\n",
        "\n",
        "As with any python program, the first steps will be to import the necessary packages. Below, import several components from the aif360 package. Import the GermanDataset, metrics to check for bias, and classes related to the algorithm I will use to mitigate bias. Also, pandas and numpy get abreviated as pd and np to facilitate use."
      ],
      "metadata": {
        "id": "ko2MNwLskLBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading all necessary packages and German credit dataset\n",
        "import pandas as pd\n",
        "\n",
        "import sys\n",
        "sys.path.insert(1, \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\")\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "from aif360.datasets import GermanDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "ZsBUTmxt1HjI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.** As in any dataset, I need to check and clean it of errors, duplications and blank (null) spaces. As it seems to be ok and did not detect errors, I can carry on."
      ],
      "metadata": {
        "id": "f_atM8yqmstJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\")\n",
        "# Check the number of rows and columns in the dataset\n",
        "print(df.shape)\n",
        "\n",
        "# Check the data types of the columns in the dataset\n",
        "print(df.dtypes)\n",
        "\n",
        "# Check for missing values in the dataset\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Remove any rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Check for duplicate rows in the dataset\n",
        "print(df.duplicated().sum())\n",
        "\n",
        "# Remove any duplicate rows in the dataset\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YcI63dCl1kKg",
        "outputId": "ed6bd8bd-8fab-437f-e723-dcca8a68e778"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(999, 1)\n",
            "A11 6 A34 A43 1169 A65 A75 4 A93 A101 4 A121 67 A143 A152 2 A173 1 A192 A201 1    object\n",
            "dtype: object\n",
            "A11 6 A34 A43 1169 A65 A75 4 A93 A101 4 A121 67 A143 A152 2 A173 1 A192 A201 1    0\n",
            "dtype: int64\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.** Because of using an online environment, Google Colab, I need to upload the two documents, german.data and german.doc, to the server. If you are using a local server (i.e., your computer), then you need to set up the file path accordingly."
      ],
      "metadata": {
        "id": "wrBuNkyYnESx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the downloaded files to Colab\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "Pk9DwWgt2rI8",
        "outputId": "cd2eac90-cb29-4a96-ecde-0fa7c107da01"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-99d5076e-ee87-469d-81ed-3dee29d41c6d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-99d5076e-ee87-469d-81ed-3dee29d41c6d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving german.data to german.data\n",
            "Saving german.doc to german.doc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.**   Load the initial dataset, setting the protected attribute to be age (as previously discussed). Then split the original dataset into training and testing datasets, so I can test outcomes comparing the two datasets. Although I will use only the training dataset in this example, a normal workflow would also use a test dataset for assessing the efficacy (accuracy, fairness, etc.) during the development of a machine learning model. Finally, set two variables (to be used in Step 6) for the privileged (1) and unprivileged (0) values for the age attribute. These are key inputs for detecting and mitigating bias, which will be Step 6 and Step 7.\n"
      ],
      "metadata": {
        "id": "ZCgnYVO7oA9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the directory path\n",
        "aif360_data_dir = '/usr/local/lib/python3.10/dist-packages/aif360/data/raw/german'\n",
        "\n",
        "# Move the uploaded files to the AIF360 data directory\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, os.path.join(aif360_data_dir, filename))"
      ],
      "metadata": {
        "id": "E0poLvsi3ACI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_orig = GermanDataset(\n",
        "    protected_attribute_names=['age'],           # this dataset also contains protected\n",
        "                                                 # attribute for \"sex\" which I do not\n",
        "                                                 # consider in this evaluation\n",
        "    privileged_classes=[lambda x: x >= 25],      # age >=25 is considered privileged\n",
        "    features_to_drop=['personal_status', 'sex'] # ignore sex-related attributes\n",
        ")\n",
        "\n",
        "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
        "\n",
        "privileged_groups = [{'age': 1}]\n",
        "unprivileged_groups = [{'age': 0}]"
      ],
      "metadata": {
        "id": "KiA3YXvQ1qKg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**6.** Compute fairness metric on original training dataset\n",
        "\n",
        "Now that I have identified the protected attribute 'age' and defined privileged and unprivileged values, I can use AIF360 to detect bias in the dataset. One simple test is to compare the percentage of favorable results for the privileged and unprivileged groups, subtracting the former percentage from the latter. A negative value indicates less favorable outcomes for the unprivileged groups. This is implemented in the method called mean_difference on the BinaryLabelDatasetMetric class. The code below performs this check and displays the output, showing that the difference is -0.169905, which shows a less favourable outcome for unprivileged groups (age < 25). It means that it is biased.\n"
      ],
      "metadata": {
        "id": "EFr2RqNjqOqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train,\n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Original training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "71U1eIlS3PTp",
        "outputId": "c0795942-2e3c-4798-e8c1-209b58f8a228"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### Original training dataset"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference in mean outcomes between unprivileged and privileged groups = -0.169905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**7.** Mitigate bias by transforming the original dataset\n",
        "\n",
        "The previous step showed that the privileged group was getting 17% more positive outcomes in the training dataset. Since this is not desirable, I am going to try to mitigate this bias in the training dataset. It is called pre-processing mitigation because it is done before the creation of the model. Bias detection and mitigation can be done before, during or after the model processes the data.\n",
        "\n",
        "AI Fairness 360 implements several pre-processing mitigation algorithms. I will choose the Reweighing algorithm [1], which is implemented in the Reweighing class in the aif360.algorithms.preprocessing package. This algorithm will transform the dataset to have more equity in positive outcomes on the protected attribute for the privileged and unprivileged groups.\n",
        "\n",
        "I then call the fit and transform methods to perform the transformation, producing a newly transformed training dataset (dataset_transf_train).\n",
        "\n",
        "[1] F. Kamiran and T. Calders, \"Data Preprocessing Techniques for Classification without Discrimination,\" Knowledge and Information Systems, 2012.\n"
      ],
      "metadata": {
        "id": "9szoYWHqrePx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
        "                privileged_groups=privileged_groups)\n",
        "dataset_transf_train = RW.fit_transform(dataset_orig_train)"
      ],
      "metadata": {
        "id": "i2ypOE303pgM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**8.** Compute fairness metric on transformed dataset\n",
        "\n",
        "Now that that the dataset is transformed, I can check how effective it was in removing bias by using the same metric I used for the original training dataset in Step 6. Once again, use the function mean_difference in the BinaryLabelDatasetMetric class to check bias. I can see the mitigation step was very effective, the difference in mean outcomes is now 0.0. So we went from a 17% advantage for the privileged group to equality in terms of mean outcome.\n"
      ],
      "metadata": {
        "id": "R7LCLKimsTjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train,\n",
        "                                               unprivileged_groups=unprivileged_groups,\n",
        "                                               privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Transformed training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "vjZ7dikr3xML",
        "outputId": "3efd9385-6d59-4d2a-dfe1-c700bd0fc7ea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### Transformed training dataset"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
          ]
        }
      ]
    }
  ]
}